{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn2gqWFG_N0J"
      },
      "source": [
        "# BART XLSX analysis pipeline (BIDS-compatible)\n",
        "This notebook reads the BART `.xlsx` files produced by your PsychoPy task (sheets: `trials`, `pumps`, `summary`).\n",
        "It outputs cleaned per-trial data, per-session metrics, and (optionally) longitudinal trends across sessions.\n"
      ],
      "id": "wn2gqWFG_N0J"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YeKPdf9_N0K"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip -q install openpyxl scipy\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n"
      ],
      "id": "8YeKPdf9_N0K"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsHNoTxK_N0K",
        "outputId": "22d668fb-0390-46ad-995e-c99172bdbb2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OUTPUT_DIR: /content/bart_data RUN_TAG: 20260120-202310\n"
          ]
        }
      ],
      "source": [
        "import os, glob, time\n",
        "OUTPUT_DIR = '/content/bart_data'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "RUN_TAG = time.strftime('%Y%m%d-%H%M%S')\n",
        "\n",
        "# Descriptive subject tag for output filenames (computed later after data load)\n",
        "def compute_sub_tag(df, col=\"sub\"):\n",
        "    import pandas as pd\n",
        "    if df is None or col not in df.columns:\n",
        "        return \"sub-UNKNOWN\"\n",
        "    subs = [str(s).upper().strip().replace(\"SUB-\",\"\") for s in df[col].dropna().unique()]\n",
        "    subs = sorted([s for s in subs if s])\n",
        "    if len(subs) == 0:\n",
        "        return \"sub-UNKNOWN\"\n",
        "    if len(subs) == 1:\n",
        "        return f\"sub-{subs[0]}\"\n",
        "    return f\"sub-{subs[0]}-{subs[-1]}_n{len(subs)}\"\n",
        "print('OUTPUT_DIR:', OUTPUT_DIR, 'RUN_TAG:', RUN_TAG)\n"
      ],
      "id": "zsHNoTxK_N0K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUqJQrOH_N0K"
      },
      "source": [
        "## 1) Upload files\n",
        "Upload **one or more** BART `.xlsx` files.\n",
        "\n",
        "**Optional:** also upload a `manifest.csv` with columns: `sub`, `condition` (NF/SHAM)."
      ],
      "id": "HUqJQrOH_N0K"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnhFWqK1_N0K",
        "outputId": "7bf8c873-67b5-4e34-cecd-0271e0391aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manifest: /content/manifest.xlsx\n",
            "BART XLSX files: 8\n",
            "/content/sub-P004_ses-S001_task-BART_run-001_beh.xlsx\n",
            "/content/sub-P004_ses-S002_task-BART_run-001_beh.xlsx\n",
            "/content/sub-P004_ses-S003_task-BART_run-001_beh.xlsx\n",
            "/content/sub-P004_ses-S004_task-BART_run-001_beh.xlsx\n",
            "/content/sub-P005_ses-S001_task-BART_run-001_beh.xlsx\n",
            "/content/sub-P005_ses-S002_task-BART_run-001_beh.xlsx\n",
            "/content/sub-P005_ses-S003_task-BART_run-001_beh.xlsx\n",
            "/content/sub-P005_ses-S004_task-BART_run-001_beh.xlsx\n",
            "First few XLSX: ['/content/sub-P004_ses-S001_task-BART_run-001_beh.xlsx', '/content/sub-P004_ses-S002_task-BART_run-001_beh.xlsx', '/content/sub-P004_ses-S003_task-BART_run-001_beh.xlsx', '/content/sub-P004_ses-S004_task-BART_run-001_beh.xlsx', '/content/sub-P005_ses-S001_task-BART_run-001_beh.xlsx', '/content/sub-P005_ses-S002_task-BART_run-001_beh.xlsx', '/content/sub-P005_ses-S003_task-BART_run-001_beh.xlsx', '/content/sub-P005_ses-S004_task-BART_run-001_beh.xlsx']\n"
          ]
        }
      ],
      "source": [
        "# Use XLSX files already present in /content (Colab file pane)\n",
        "import glob, os\n",
        "\n",
        "# only BART files (BIDS-like)\n",
        "xlsx_files = sorted([p for p in glob.glob(\"/content/*.xlsx\")\n",
        "                     if \"sub-\" in os.path.basename(p).lower()])\n",
        "\n",
        "manifest_csv = sorted(glob.glob(\"/content/manifest*.csv\"))\n",
        "manifest_xlsx = sorted(glob.glob(\"/content/manifest*.xlsx\"))\n",
        "\n",
        "manifest_path = manifest_csv[0] if manifest_csv else (manifest_xlsx[0] if manifest_xlsx else None)\n",
        "print(\"Manifest:\", manifest_path)\n",
        "\n",
        "\n",
        "print(\"BART XLSX files:\", len(xlsx_files))\n",
        "print(\"\\n\".join(xlsx_files[:10]))\n",
        "\n",
        "print('First few XLSX:', xlsx_files[:10])\n",
        "\n",
        "assert len(xlsx_files) > 0, 'No .xlsx files found in /content. Drag them into the Colab file pane first.'\n"
      ],
      "id": "DnhFWqK1_N0K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTL9Pbed_N0K"
      },
      "source": [
        "## 2) Helpers: parse BIDS filename + load sheets"
      ],
      "id": "mTL9Pbed_N0K"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb230THB_N0K"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "BIDS_PAT = re.compile(\n",
        "    r\"sub-(?P<sub>[^_]+)_ses-(?P<ses>[^_]+)_task-(?P<task>[^_]+)_run-(?P<run>[^_]+)_beh\\.xlsx$\",\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "def parse_bids(fn: str):\n",
        "    m = BIDS_PAT.search(fn.replace(\" \", \"\"))\n",
        "    if not m:\n",
        "        # fallback: try without _beh\n",
        "        m2 = re.search(r\"sub-([^_]+)_ses-([^_]+)_task-([^_]+)_run-([^_]+)\", fn, re.IGNORECASE)\n",
        "        if not m2:\n",
        "            raise ValueError(f\"Filename not BIDS-like: {fn}\")\n",
        "        return {\"sub\": m2.group(1), \"ses\": m2.group(2), \"task\": m2.group(3), \"run\": m2.group(4)}\n",
        "    return {k: v.upper() for k, v in m.groupdict().items()}\n",
        "\n",
        "def read_bart_xlsx(fn: str):\n",
        "    meta = parse_bids(fn)\n",
        "    xl = pd.ExcelFile(fn)\n",
        "    trials = pd.read_excel(xl, sheet_name=\"trials\") if \"trials\" in xl.sheet_names else pd.DataFrame()\n",
        "    pumps  = pd.read_excel(xl, sheet_name=\"pumps\")  if \"pumps\"  in xl.sheet_names else None\n",
        "    summary= pd.read_excel(xl, sheet_name=\"summary\") if \"summary\" in xl.sheet_names else None\n",
        "\n",
        "    # Attach filename meta (in case your sheet doesn't include it)\n",
        "    for df in [trials, pumps, summary]:\n",
        "        if df is None or len(df)==0:\n",
        "            continue\n",
        "        for k,v in meta.items():\n",
        "            if k not in df.columns:\n",
        "                df[k]=v\n",
        "        df[\"file\"]=fn\n",
        "    return meta, trials, pumps, summary\n",
        "\n",
        "def to_numeric_safe(df, cols):\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c]=pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df\n"
      ],
      "id": "Rb230THB_N0K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7woxnQv_N0L"
      },
      "source": [
        "## 3) Load all XLSX into one table"
      ],
      "id": "V7woxnQv_N0L"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mX0Ri4z_N0L",
        "outputId": "7ffe2811-9285-44a6-e45c-e99b0f2bdc69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trials rows: 360\n",
            "Pumps rows : 5022\n",
            "Summary rows: 16\n",
            "SUB_TAG: sub-P004-P005_n2\n"
          ]
        }
      ],
      "source": [
        "all_trials=[]\n",
        "all_pumps=[]\n",
        "all_summary=[]\n",
        "\n",
        "for fn in xlsx_files:\n",
        "    meta, trials, pumps, summary = read_bart_xlsx(fn)\n",
        "    if len(trials):\n",
        "        all_trials.append(trials)\n",
        "    if pumps is not None and len(pumps):\n",
        "        all_pumps.append(pumps)\n",
        "    if summary is not None and len(summary):\n",
        "        all_summary.append(summary)\n",
        "\n",
        "trials_df = pd.concat(all_trials, ignore_index=True) if all_trials else pd.DataFrame()\n",
        "pumps_df  = pd.concat(all_pumps,  ignore_index=True) if all_pumps  else None\n",
        "summary_df= pd.concat(all_summary,ignore_index=True) if all_summary else None\n",
        "\n",
        "print(\"Trials rows:\", len(trials_df))\n",
        "print(\"Pumps rows :\", 0 if pumps_df is None else len(pumps_df))\n",
        "print(\"Summary rows:\", 0 if summary_df is None else len(summary_df))\n",
        "\n",
        "trials_df.head()\n",
        "\n",
        "\n",
        "# Compute descriptive subject tag once data are loaded\n",
        "SUB_TAG = compute_sub_tag(trials_df, col='sub')\n",
        "print('SUB_TAG:', SUB_TAG)\n"
      ],
      "id": "1mX0Ri4z_N0L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAATyJ-i_N0L"
      },
      "source": [
        "## 4) Optional: merge condition (NF vs SHAM) from manifest\n",
        "If you uploaded a `manifest.csv` with columns `sub, condition`, it will be joined by `sub`.\n",
        "If you did not upload one, condition is left blank."
      ],
      "id": "tAATyJ-i_N0L"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "sjR0hUuC_N0L",
        "outputId": "1b66807c-6e91-45fe-ef15-728ebd7b4f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'manifest_files' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3512807500.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mmanifest_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanifest_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# accept either 'sub' or 'subject' naming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"sub\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"subject\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'manifest_files' is not defined"
          ]
        }
      ],
      "source": [
        "if manifest_files:\n",
        "    mf = pd.read_csv(manifest_files[0])\n",
        "    mf.columns = [c.strip().lower() for c in mf.columns]\n",
        "    # accept either 'sub' or 'subject' naming\n",
        "    if \"sub\" not in mf.columns and \"subject\" in mf.columns:\n",
        "        mf = mf.rename(columns={\"subject\":\"sub\"})\n",
        "    if \"condition\" in mf.columns:\n",
        "        mf[\"condition\"]=mf[\"condition\"].astype(str).str.upper().str.strip()\n",
        "    mf[\"sub\"]=mf[\"sub\"].astype(str).str.upper().str.strip().str.replace(\"^SUB-\",\"\", regex=True)\n",
        "\n",
        "    trials_df[\"sub\"]=trials_df[\"sub\"].astype(str).str.upper().str.strip().str.replace(\"^SUB-\",\"\", regex=True)\n",
        "    trials_df = trials_df.merge(mf[[\"sub\",\"condition\"]], on=\"sub\", how=\"left\")\n",
        "    if pumps_df is not None:\n",
        "        pumps_df[\"sub\"]=pumps_df[\"sub\"].astype(str).str.upper().str.strip().str.replace(\"^SUB-\",\"\", regex=True)\n",
        "        pumps_df = pumps_df.merge(mf[[\"sub\",\"condition\"]], on=\"sub\", how=\"left\")\n",
        "else:\n",
        "    trials_df[\"condition\"] = np.nan\n",
        "    if pumps_df is not None:\n",
        "        pumps_df[\"condition\"] = np.nan\n",
        "\n",
        "trials_df[[\"sub\",\"ses\",\"run\",\"task\",\"condition\",\"file\"]].drop_duplicates().head(10)\n"
      ],
      "id": "sjR0hUuC_N0L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLdTNqLE_N0L"
      },
      "source": [
        "## 5) Clean trials + compute session metrics\n",
        "We compute (Main block):\n",
        "- mean adjusted pumps (non-exploded trials only)\n",
        "- explosion frequency\n",
        "- final bank (max total_earnings)\n",
        "- pump latency (median/mean)\n",
        "- resting theta pre/post if present\n"
      ],
      "id": "OLdTNqLE_N0L"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wtd9wIoH_N0L"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "df = trials_df.copy()\n",
        "\n",
        "# numeric conversions (safe if missing)\n",
        "df = to_numeric_safe(df, [\n",
        "    \"exploded_int\",\"pump_count\",\"adjusted_pumps_trial\",\n",
        "    \"pump_latency_first\",\"pump_latency_mean\",\"pump_latency_median\",\n",
        "    \"trial_duration_sec\",\"trial_earnings\",\"total_earnings\",\n",
        "    \"rest_pre_eo_theta_mean\",\"rest_pre_ec_theta_mean\",\n",
        "    \"rest_post_eo_theta_mean\",\"rest_post_ec_theta_mean\",\n",
        "    \"baseline_mu\",\"baseline_sigma\",\"baseline_n\"\n",
        "])\n",
        "\n",
        "# standardize block filtering\n",
        "df[\"block\"] = df[\"block\"].astype(str)\n",
        "\n",
        "# Valid-trial filter (tweak as needed)\n",
        "if \"trial_duration_sec\" in df.columns:\n",
        "    df[\"valid_trial\"] = df[\"trial_duration_sec\"].between(2, 180, inclusive=\"both\")\n",
        "else:\n",
        "    df[\"valid_trial\"] = True\n",
        "\n",
        "# Latency validity window (decision hesitancy)\n",
        "LAT_MIN, LAT_MAX = 0.15, 6.0\n",
        "if \"pump_latency_median\" in df.columns:\n",
        "    df[\"latency_ok\"] = df[\"pump_latency_median\"].between(LAT_MIN, LAT_MAX, inclusive=\"both\")\n",
        "else:\n",
        "    df[\"latency_ok\"] = True\n",
        "\n",
        "# focus on Main block\n",
        "main = df[df[\"block\"].str.lower().eq(\"main\") & df[\"valid_trial\"]].copy()\n",
        "\n",
        "# adjusted pumps: only non-exploded trials\n",
        "if {\"exploded_int\",\"adjusted_pumps_trial\"}.issubset(main.columns):\n",
        "    main[\"adjusted_pumps_clean\"] = np.where(\n",
        "        (main[\"exploded_int\"]==0) & main[\"latency_ok\"],\n",
        "        main[\"adjusted_pumps_trial\"],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "# Session-level metrics per (sub,ses,run)\n",
        "group_cols = [\"sub\",\"ses\",\"run\",\"task\",\"condition\",\"file\"]\n",
        "def session_metrics(g):\n",
        "    out={}\n",
        "    out[\"n_trials_main\"] = len(g)\n",
        "    out[\"explosion_frequency\"] = g[\"exploded_int\"].mean() if \"exploded_int\" in g.columns else np.nan\n",
        "    out[\"mean_adjusted_pumps\"] = g[\"adjusted_pumps_clean\"].mean() if \"adjusted_pumps_clean\" in g.columns else np.nan\n",
        "    out[\"median_pump_latency\"] = g[\"pump_latency_median\"].median() if \"pump_latency_median\" in g.columns else np.nan\n",
        "    out[\"mean_pump_latency\"] = g[\"pump_latency_mean\"].mean() if \"pump_latency_mean\" in g.columns else np.nan\n",
        "    out[\"final_bank\"] = g[\"total_earnings\"].max() if \"total_earnings\" in g.columns else np.nan\n",
        "\n",
        "    # Pull rest/baseline values (they're repeated across rows; take first non-null)\n",
        "    for col in [\"baseline_mu\",\"baseline_sigma\",\"baseline_n\",\n",
        "                \"rest_pre_eo_theta_mean\",\"rest_pre_ec_theta_mean\",\n",
        "                \"rest_post_eo_theta_mean\",\"rest_post_ec_theta_mean\"]:\n",
        "        if col in g.columns:\n",
        "            s = g[col].dropna()\n",
        "            out[col] = s.iloc[0] if len(s) else np.nan\n",
        "        else:\n",
        "            out[col] = np.nan\n",
        "    return pd.Series(out)\n",
        "\n",
        "sessions = main.groupby(group_cols, dropna=False).apply(session_metrics).reset_index()\n",
        "sessions = sessions.sort_values([\"sub\",\"ses\",\"run\"])\n",
        "sessions.head(10)\n"
      ],
      "id": "Wtd9wIoH_N0L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61l1AqFI_N0L"
      },
      "source": [
        "## 6) Longitudinal trends across sessions (optional)\n",
        "If each subject has multiple sessions, compute a simple **slope per subject** over session number.\n",
        "This works even if you keep condition blinded (or absent)."
      ],
      "id": "61l1AqFI_N0L"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eiIuR_X_N0L"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Try to convert ses like 'S032' -> 32 for ordering\n",
        "def ses_to_int(s):\n",
        "    s = str(s)\n",
        "    m = re.search(r\"(\\d+)\", s)\n",
        "    return int(m.group(1)) if m else np.nan\n",
        "\n",
        "sessions[\"ses_num\"] = sessions[\"ses\"].apply(ses_to_int)\n",
        "\n",
        "def slope_over_sessions(g, metric):\n",
        "    d = g.sort_values(\"ses_num\")\n",
        "    y = d[metric].astype(float).values\n",
        "    x = d[\"ses_num\"].astype(float).values\n",
        "    mask = ~np.isnan(x) & ~np.isnan(y)\n",
        "    if mask.sum() < 3:\n",
        "        return np.nan\n",
        "    X = np.vstack([x[mask], np.ones(mask.sum())]).T\n",
        "    b, a = np.linalg.lstsq(X, y[mask], rcond=None)[0]  # y = b*x + a\n",
        "    return b\n",
        "\n",
        "slopes=[]\n",
        "for (sub, cond), g in sessions.groupby([\"sub\",\"condition\"], dropna=False):\n",
        "    slopes.append({\n",
        "        \"sub\": sub,\n",
        "        \"condition\": cond,\n",
        "        \"n_sessions\": g[\"ses_num\"].nunique(),\n",
        "        \"slope_latency\": slope_over_sessions(g, \"median_pump_latency\"),\n",
        "        \"slope_adjusted\": slope_over_sessions(g, \"mean_adjusted_pumps\"),\n",
        "        \"slope_explosion\": slope_over_sessions(g, \"explosion_frequency\"),\n",
        "        \"slope_bank\": slope_over_sessions(g, \"final_bank\"),\n",
        "    })\n",
        "slopes_df = pd.DataFrame(slopes)\n",
        "slopes_df.head(10)\n"
      ],
      "id": "7eiIuR_X_N0L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ2yGP67_N0L"
      },
      "source": [
        "## 7) Quick plots"
      ],
      "id": "AJ2yGP67_N0L"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-w1EQsf_N0L"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def plot_metric(metric):\n",
        "    plt.figure()\n",
        "    for label, g in sessions.groupby('condition', dropna=False):\n",
        "        means = g.groupby('ses_num')[metric].mean()\n",
        "        plt.plot(means.index.values, means.values, marker='o', label=str(label))\n",
        "    plt.xlabel('Session')\n",
        "    plt.ylabel(metric)\n",
        "    plt.title(f'{metric} across sessions')\n",
        "    plt.legend()\n",
        "    # Save figure into bart_data instead of downloading\n",
        "    out_png = os.path.join(OUTPUT_DIR, f'bart_{metric}_across_sessions_{SUB_TAG}_{RUN_TAG}.png')\n",
        "    plt.savefig(out_png, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "for m in ['mean_adjusted_pumps','explosion_frequency','median_pump_latency','final_bank']:\n",
        "    if m in sessions.columns:\n",
        "        plot_metric(m)\n"
      ],
      "id": "l-w1EQsf_N0L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCJMEll3_N0M"
      },
      "source": [
        "## 8) Export combined tables"
      ],
      "id": "cCJMEll3_N0M"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFg46fyI_N0M"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Write cleaned tables into /content/bart_data with descriptive filenames\n",
        "trials_out   = os.path.join(OUTPUT_DIR, f'bart_trials_main_clean_{SUB_TAG}_{SUB_TAG}_{RUN_TAG}.csv')\n",
        "sessions_out = os.path.join(OUTPUT_DIR, f'bart_sessions_metrics_{SUB_TAG}_{SUB_TAG}_{RUN_TAG}.csv')\n",
        "slopes_out   = os.path.join(OUTPUT_DIR, f'bart_subject_slopes_{SUB_TAG}_{SUB_TAG}_{RUN_TAG}.csv')\n",
        "\n",
        "main.to_csv(trials_out, index=False)\n",
        "sessions.to_csv(sessions_out, index=False)\n",
        "slopes_df.to_csv(slopes_out, index=False)\n",
        "\n",
        "print('Wrote:')\n",
        "print(' -', trials_out)\n",
        "print(' -', sessions_out)\n",
        "print(' -', slopes_out)\n"
      ],
      "id": "iFg46fyI_N0M"
    }
  ]
}