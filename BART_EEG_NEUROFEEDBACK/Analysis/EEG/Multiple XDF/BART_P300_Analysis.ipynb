{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "slObE7EuGROS",
      "metadata": {
        "id": "slObE7EuGROS"
      },
      "source": [
        "# BART → P300 (Explosion-locked) ERP pipeline (XDF → CSV/XLSX)\n",
        "\n",
        "This notebook:\n",
        "- loads one or more **LabRecorder `.xdf`** files\n",
        "- finds **EEG** + **BART_Markers** streams\n",
        "- extracts **BART_EXPLODE** events and epochs EEG around them\n",
        "- computes per-explosion **P300 amplitude + latency**\n",
        "- exports `p300_explosions.csv` / `p300_explosions.xlsx` and a `p300_session_summary.csv`\n",
        "\n",
        "**Best practice:** keep your BART task sending the `BART_EXPLODE` marker on the same frame as the BOOM/flash visual.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "LX85Guc1GROW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX85Guc1GROW",
        "outputId": "9fc550ac-c91d-4bf2-c54f-aa47329e2e61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m5.6/7.5 MB\u001b[0m \u001b[31m169.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.4/7.5 MB\u001b[0m \u001b[31m151.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install pyxdf mne openpyxl\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import pyxdf\n",
        "import mne\n",
        "\n",
        "mne.set_log_level(\"WARNING\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R34R44STGROX",
      "metadata": {
        "id": "R34R44STGROX"
      },
      "source": [
        "## Upload XDF files\n",
        "Choose multiple `.xdf` files in the uploader (or drag-and-drop into the Files pane).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "NbkqWvyFGROX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "NbkqWvyFGROX",
        "outputId": "28c0b273-e08a-4642-bd1b-27a0970d64fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XDF files: []\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Upload at least one .xdf file to /content.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2362738973.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"XDF files:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxdf_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxdf_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Upload at least one .xdf file to /content.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: Upload at least one .xdf file to /content."
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Find all .xdf files in /content\n",
        "xdf_files = sorted([str(p) for p in Path(\"/content\").glob(\"*.xdf\")])\n",
        "\n",
        "print(\"XDF files:\", xdf_files)\n",
        "assert len(xdf_files) > 0, \"Upload at least one .xdf file to /content.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GgqIyyUtGROX",
      "metadata": {
        "id": "GgqIyyUtGROX"
      },
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mqtJN_QUGROY",
      "metadata": {
        "id": "mqtJN_QUGROY"
      },
      "outputs": [],
      "source": [
        "# ---------------- USER CONFIG (ActiCAP 16ch montage) ----------------\n",
        "\n",
        "# Streams\n",
        "EEG_STREAM_NAME_CANDIDATES = [\"openvibeSignal\", \"EEG\", \"ActiCAP\", \"BrainVision\"]\n",
        "MARKER_STREAM_NAME = \"BART_Markers\"\n",
        "\n",
        "# Sampling (your cap/diagram uses 512 Hz; we still read nominal_srate from XDF if present)\n",
        "EXPECTED_SFREQ = 512.0\n",
        "\n",
        "# Epoching around explosions\n",
        "TMIN = -0.200\n",
        "TMAX =  0.800\n",
        "BASELINE = (-0.200, 0.0)\n",
        "\n",
        "# P300 windows\n",
        "P300_WIN = (0.250, 0.500)        # peak window\n",
        "P300_MEAN_WIN = (0.300, 0.450)   # mean window (often more stable)\n",
        "\n",
        "# Channel map (1..16) — corrected labels where \"2\" in the drawing means \"z\"\n",
        "#  1 Fz,  2 Cz,  3 Pz,  4 POz,\n",
        "#  5 Fp1, 6 Fp2, 7 F3,  8 F4,\n",
        "#  9 FCz, 10 C3, 11 C4, 12 CPz,\n",
        "#  13 P3, 14 P4, 15 O1, 16 O2\n",
        "ACTICAP_16_CH_NAMES = [\n",
        "    \"Fz\", \"Cz\", \"Pz\", \"POz\",\n",
        "    \"Fp1\", \"Fp2\", \"F3\", \"F4\",\n",
        "    \"FCz\", \"C3\", \"C4\", \"CPz\",\n",
        "    \"P3\", \"P4\", \"O1\", \"O2\"\n",
        "]\n",
        "\n",
        "# P300 channel preference / ROI\n",
        "P300_CHANNEL_PREFERENCE = [\"Pz\", \"CPz\", \"POz\", \"Cz\", \"P3\", \"P4\"]\n",
        "P300_ROI = [\"Pz\", \"CPz\", \"P3\", \"P4\", \"POz\"]\n",
        "\n",
        "# Artifact rejection (simple peak-to-peak); set None to disable\n",
        "REJECT_UV = 120.0\n",
        "REJECT = None if REJECT_UV is None else dict(eeg=REJECT_UV * 1e-6)\n",
        "\n",
        "print(\"Reject:\", REJECT)\n",
        "print(\"ActiCAP16 channel map:\", ACTICAP_16_CH_NAMES)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yT5xriOsGROY",
      "metadata": {
        "id": "yT5xriOsGROY"
      },
      "source": [
        "## Helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AefX3B3-GROY",
      "metadata": {
        "id": "AefX3B3-GROY"
      },
      "outputs": [],
      "source": [
        "def parse_bids_from_xdf_filename(xdf_path: str):\n",
        "    # Parse sub/ses/run/task if filename is BIDS-like:\n",
        "    # sub-P001_ses-S032_task-Default_run-001_eeg.xdf\n",
        "    name = Path(xdf_path).name\n",
        "    out = {\"sub\": None, \"ses\": None, \"run\": None, \"task\": None, \"file\": name}\n",
        "    m = re.search(r\"sub-([A-Za-z0-9]+)\", name)\n",
        "    if m: out[\"sub\"] = m.group(1)\n",
        "    m = re.search(r\"ses-([A-Za-z0-9]+)\", name)\n",
        "    if m: out[\"ses\"] = m.group(1)\n",
        "    m = re.search(r\"run-([A-Za-z0-9]+)\", name)\n",
        "    if m: out[\"run\"] = m.group(1)\n",
        "    m = re.search(r\"task-([A-Za-z0-9]+)\", name)\n",
        "    if m: out[\"task\"] = m.group(1)\n",
        "    return out\n",
        "\n",
        "def find_stream(streams, want_name=None, want_type=None):\n",
        "    for s in streams:\n",
        "        info = s[\"info\"]\n",
        "        name = info.get(\"name\", [\"\"])[0]\n",
        "        stype = info.get(\"type\", [\"\"])[0]\n",
        "        if want_name is not None and name == want_name:\n",
        "            return s\n",
        "        if want_type is not None and stype == want_type:\n",
        "            return s\n",
        "    return None\n",
        "\n",
        "def find_eeg_stream(streams):\n",
        "    for nm in EEG_STREAM_NAME_CANDIDATES:\n",
        "        s = find_stream(streams, want_name=nm)\n",
        "        if s is not None:\n",
        "            return s\n",
        "    s = find_stream(streams, want_type=\"EEG\")\n",
        "    if s is not None:\n",
        "        return s\n",
        "    # fallback: choose the largest numeric 2D stream\n",
        "    best, best_n = None, -1\n",
        "    for s in streams:\n",
        "        ts = s.get(\"time_series\", None)\n",
        "        if ts is None:\n",
        "            continue\n",
        "        arr = np.asarray(ts)\n",
        "        if arr.ndim == 2 and arr.shape[1] >= 4 and arr.shape[1] > best_n:\n",
        "            best, best_n = s, arr.shape[1]\n",
        "    return best\n",
        "\n",
        "def parse_marker_strings(marker_stream):\n",
        "    msgs = marker_stream[\"time_series\"]\n",
        "    ts = marker_stream[\"time_stamps\"]\n",
        "    out = []\n",
        "    for t, m in zip(ts, msgs):\n",
        "        if isinstance(m, (list, tuple, np.ndarray)):\n",
        "            m = m[0] if len(m) else \"\"\n",
        "        if isinstance(m, bytes):\n",
        "            m = m.decode(\"utf-8\", errors=\"ignore\")\n",
        "        out.append((float(t), str(m)))\n",
        "    return out\n",
        "\n",
        "def nearest_sample_index(eeg_t, event_t):\n",
        "    return int(np.argmin(np.abs(eeg_t - event_t)))\n",
        "\n",
        "def compute_p300_features(epoch_1d, sfreq):\n",
        "    '''\n",
        "    epoch_1d: (n_times,) in Volts\n",
        "\n",
        "    Returns:\n",
        "      peak_amp_V, peak_lat_s, mean_amp_V\n",
        "\n",
        "    - peak is the max within P300_WIN\n",
        "    - mean is the average within P300_MEAN_WIN (often more stable than a peak)\n",
        "    '''\n",
        "    t = np.arange(epoch_1d.size) / sfreq + TMIN\n",
        "\n",
        "    # --- peak within P300_WIN ---\n",
        "    w0, w1 = P300_WIN\n",
        "    mask = (t >= w0) & (t <= w1)\n",
        "    if not mask.any():\n",
        "        peak_amp, peak_lat = np.nan, np.nan\n",
        "    else:\n",
        "        seg = epoch_1d[mask]\n",
        "        i_peak = int(np.argmax(seg))\n",
        "        peak_amp = float(seg[i_peak])\n",
        "        peak_lat = float(t[mask][i_peak])\n",
        "\n",
        "    # --- mean within P300_MEAN_WIN ---\n",
        "    m0, m1 = P300_MEAN_WIN\n",
        "    mmask = (t >= m0) & (t <= m1)\n",
        "    mean_amp = float(np.mean(epoch_1d[mmask])) if mmask.any() else np.nan\n",
        "\n",
        "    return peak_amp, peak_lat, mean_amp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZbmM1E7lGROZ",
      "metadata": {
        "id": "ZbmM1E7lGROZ"
      },
      "source": [
        "## Extract explosions + compute P300 (per explosion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CcUI3AdgGROZ",
      "metadata": {
        "id": "CcUI3AdgGROZ"
      },
      "outputs": [],
      "source": [
        "rows = []\n",
        "\n",
        "for xdf_path in xdf_files:\n",
        "    bids = parse_bids_from_xdf_filename(xdf_path)\n",
        "    print(\"\\n=== Loading:\", xdf_path, \"===\")\n",
        "    streams, header = pyxdf.load_xdf(xdf_path)\n",
        "\n",
        "    eeg_stream = find_eeg_stream(streams)\n",
        "    marker_stream = find_stream(streams, want_name=MARKER_STREAM_NAME)\n",
        "\n",
        "    if eeg_stream is None:\n",
        "        print(\"⚠️ No EEG stream found. Skipping.\")\n",
        "        continue\n",
        "    if marker_stream is None:\n",
        "        print(\"⚠️ No marker stream named BART_Markers found. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    eeg = np.asarray(eeg_stream[\"time_series\"])\n",
        "    eeg_t = np.asarray(eeg_stream[\"time_stamps\"])\n",
        "    sfreq = float(eeg_stream[\"info\"].get(\"nominal_srate\", [0])[0] or 0)\n",
        "\n",
        "    if not sfreq or sfreq <= 0:\n",
        "        sfreq = 1.0 / np.median(np.diff(eeg_t))\n",
        "\n",
        "    print(\"EEG shape:\", eeg.shape, \"sfreq~\", sfreq)\n",
        "\n",
        "    # Try to extract channel labels\n",
        "    ch_names = None\n",
        "    try:\n",
        "        desc = eeg_stream[\"info\"][\"desc\"][0]\n",
        "        if \"channels\" in desc and \"channel\" in desc[\"channels\"][0]:\n",
        "            ch = desc[\"channels\"][0][\"channel\"]\n",
        "            labels = []\n",
        "            for c in ch:\n",
        "                if \"label\" in c and len(c[\"label\"]):\n",
        "                    labels.append(c[\"label\"][0])\n",
        "            if len(labels) == eeg.shape[1]:\n",
        "                ch_names = labels\n",
        "    except Exception:\n",
        "        ch_names = None\n",
        "\n",
        "    if ch_names is None:\n",
        "        if eeg.shape[1] == 16:\n",
        "            ch_names = ACTICAP_16_CH_NAMES\n",
        "            print(\"✅ Applied ActiCAP 16-channel label map.\")\n",
        "        else:\n",
        "            ch_names = [f\"Ch{i+1}\" for i in range(eeg.shape[1])]\n",
        "            print(\"⚠️ No channel labels; using Ch1..\")\n",
        "\n",
        "    data = eeg.T.astype(float)  # (n_ch, n_times)\n",
        "\n",
        "    # Heuristic: if values look like µV, convert to V\n",
        "    if np.median(np.abs(data)) > 1e-3:\n",
        "        data = data * 1e-6\n",
        "\n",
        "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=\"eeg\")\n",
        "    raw = mne.io.RawArray(data, info, verbose=\"ERROR\")\n",
        "\n",
        "    markers = parse_marker_strings(marker_stream)\n",
        "    explode = [(t, msg) for (t, msg) in markers if msg.startswith(\"BART_EXPLODE\")]\n",
        "    print(\"Explosions found:\", len(explode))\n",
        "    if len(explode) == 0:\n",
        "        continue\n",
        "\n",
        "    event_samps = [nearest_sample_index(eeg_t, t) for (t, _) in explode]\n",
        "    events = np.column_stack([event_samps, np.zeros(len(event_samps), dtype=int), np.ones(len(event_samps), dtype=int)])\n",
        "\n",
        "    epochs = mne.Epochs(raw, events, event_id={\"explode\": 1},\n",
        "                        tmin=TMIN, tmax=TMAX, baseline=BASELINE,\n",
        "                        reject=REJECT, preload=True, verbose=\"ERROR\")\n",
        "\n",
        "    available = set(epochs.ch_names)\n",
        "    pick = next((ch for ch in P300_CHANNEL_PREFERENCE if ch in available), None)\n",
        "    roi = [ch for ch in P300_ROI if ch in available]\n",
        "    use_roi = (pick is None and len(roi) > 0)\n",
        "\n",
        "    if pick is None and not use_roi:\n",
        "        pick = \"Cz\" if \"Cz\" in available else epochs.ch_names[0]\n",
        "        print(\"⚠️ Using fallback channel:\", pick)\n",
        "\n",
        "    for i, ep in enumerate(epochs):\n",
        "        msg = explode[i][1]\n",
        "        meta = {}\n",
        "        parts = msg.split(\";\")\n",
        "        for p in parts[1:]:\n",
        "            if \"=\" in p:\n",
        "                k, v = p.split(\"=\", 1)\n",
        "                meta[k.strip()] = v.strip()\n",
        "\n",
        "        if use_roi:\n",
        "            idxs = [epochs.ch_names.index(ch) for ch in roi]\n",
        "            ep_1d = ep[idxs, :].mean(axis=0)\n",
        "            ch_used = \"ROI:\" + \",\".join(roi)\n",
        "        else:\n",
        "            ep_1d = ep[epochs.ch_names.index(pick), :]\n",
        "            ch_used = pick\n",
        "\n",
        "        peak_amp, peak_lat, mean_amp = compute_p300_features(ep_1d, sfreq)\n",
        "\n",
        "        rows.append({\n",
        "            **bids,\n",
        "            \"event_index\": int(i),\n",
        "            \"event_time_lsl\": float(explode[i][0]),\n",
        "            \"event_sample\": int(event_samps[i]),\n",
        "            \"channel_used\": ch_used,\n",
        "            \"p300_peak_amp_V\": peak_amp,\n",
        "            \"p300_peak_amp_uV\": (peak_amp * 1e6 if np.isfinite(peak_amp) else np.nan),\n",
        "            \"p300_peak_lat_s\": peak_lat,\n",
        "            \"p300_mean_amp_V\": mean_amp,\n",
        "            \"p300_mean_amp_uV\": (mean_amp * 1e6 if np.isfinite(mean_amp) else np.nan),\n",
        "            \"block\": meta.get(\"block\", \"\"),\n",
        "            \"trial\": meta.get(\"trial\", \"\"),\n",
        "            \"pump\": meta.get(\"pump\", \"\"),\n",
        "            \"loss\": meta.get(\"loss\", \"\"),\n",
        "            \"total\": meta.get(\"total\", \"\")\n",
        "        })\n",
        "\n",
        "p300_df = pd.DataFrame(rows)\n",
        "print(\"Rows:\", len(p300_df))\n",
        "p300_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m7lAlff3GROZ",
      "metadata": {
        "id": "m7lAlff3GROZ"
      },
      "source": [
        "## Export results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1hwom3XoGROa",
      "metadata": {
        "id": "1hwom3XoGROa"
      },
      "outputs": [],
      "source": [
        "out_csv = \"p300_explosions.csv\"\n",
        "out_xlsx = \"p300_explosions.xlsx\"\n",
        "\n",
        "p300_df.to_csv(out_csv, index=False)\n",
        "\n",
        "with pd.ExcelWriter(out_xlsx, engine=\"openpyxl\") as w:\n",
        "    p300_df.to_excel(w, sheet_name=\"explosions\", index=False)\n",
        "\n",
        "print(\"Saved:\", out_csv, out_xlsx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tu17XrDpGROa",
      "metadata": {
        "id": "Tu17XrDpGROa"
      },
      "source": [
        "## Session summary (for plotting across sessions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MURMFLVuGROa",
      "metadata": {
        "id": "MURMFLVuGROa"
      },
      "outputs": [],
      "source": [
        "if len(p300_df) == 0:\n",
        "    session_df = pd.DataFrame()\n",
        "else:\n",
        "    session_df = (p300_df\n",
        "                  .groupby([\"sub\",\"ses\",\"run\",\"task\"], dropna=False)\n",
        "                  .agg(\n",
        "                      n_explosions=(\"p300_peak_amp_uV\",\"size\"),\n",
        "                      p300_peak_amp_uV_mean=(\"p300_peak_amp_uV\",\"mean\"),\n",
        "                      p300_peak_amp_uV_median=(\"p300_peak_amp_uV\",\"median\"),\n",
        "                      p300_mean_amp_uV_mean=(\"p300_mean_amp_uV\",\"mean\"),\n",
        "                      p300_mean_amp_uV_median=(\"p300_mean_amp_uV\",\"median\"),\n",
        "                      p300_peak_lat_ms_mean=(\"p300_peak_lat_s\", lambda x: np.nanmean(x)*1000.0),\n",
        "                      p300_peak_lat_ms_median=(\"p300_peak_lat_s\", lambda x: np.nanmedian(x)*1000.0),\n",
        "                      file=(\"file\",\"first\"),\n",
        "                      channel_used=(\"channel_used\",\"first\")\n",
        "                  )\n",
        "                  .reset_index())\n",
        "\n",
        "session_df.to_csv(\"p300_session_summary.csv\", index=False)\n",
        "session_df.head()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}